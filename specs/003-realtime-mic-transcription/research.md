# Research: Real-time Microphone Voice Transcription

**Feature**: 003-realtime-mic-transcription
**Research Date**: 2025-10-29
**Status**: Complete

---

## Overview

This document consolidates research findings for implementing real-time microphone voice transcription using OpenAI's gpt-4o-mini-transcribe model. Research focused on six key areas: streamlit-webrtc integration, OpenAI Audio Transcription API usage, audio processing pipeline, Voice Activity Detection, thread safety patterns, and testing strategies.

---

## R1: streamlit-webrtc Integration

### Decision

**Use streamlit-webrtc with SENDONLY mode for audio-only streaming with callback-based processing**

### Rationale

1. **Native Streamlit Integration**: Designed specifically for Streamlit applications with built-in session state compatibility
2. **WebRTC Standards Compliance**: Uses browser's native getUserMedia API for microphone access
3. **Frame-level Control**: Provides audio frame callbacks for real-time processing without buffering delays
4. **Production Ready**: Mature library with proven track record in production Streamlit apps
5. **Simple Configuration**: Minimal setup required for audio-only use case

### Alternatives Considered

**JavaScript-based AudioWorklet with Streamlit components**:
- ‚ùå Requires custom component development
- ‚ùå Complex bidirectional communication with Python backend
- ‚ùå Higher maintenance burden
- ‚úÖ Lower latency potential
- ‚úÖ More control over audio processing

**PyAudio for direct microphone capture**:
- ‚ùå Requires server-side microphone access (incompatible with web deployment)
- ‚ùå Cannot access client browser's microphone
- ‚ùå Security issues with server microphone access
- ‚úÖ Simpler audio pipeline
- ‚úÖ No WebRTC complexity

**Browser MediaRecorder API with manual chunking**:
- ‚ùå Requires custom Streamlit component
- ‚ùå Manual chunk management complexity
- ‚ùå Less precise timing control
- ‚úÖ Native browser support
- ‚úÖ Built-in audio encoding

### Implementation Pattern

```python
from streamlit_webrtc import webrtc_streamer, WebRtcMode, RTCConfiguration
import av
import numpy as np

# RTC configuration for NAT traversal
rtc_configuration = RTCConfiguration({
    "iceServers": [{"urls": ["stun:stun.l.google.com:19302"]}]
})

def audio_frame_callback(frame: av.AudioFrame) -> av.AudioFrame:
    """
    Process incoming audio frames from browser microphone.

    Args:
        frame: PyAV AudioFrame at 48000Hz (WebRTC standard)

    Returns:
        Original frame (required by streamlit-webrtc)
    """
    # Convert to NumPy array
    pcm = frame.to_ndarray()

    # Mix to mono if stereo
    if pcm.ndim == 2:
        pcm = pcm.mean(axis=0)

    # Convert to int16 PCM format
    pcm = pcm.astype(np.int16, copy=False)

    # Process audio chunk (accumulate, detect voice, etc.)
    # ... processing logic ...

    return frame

# Initialize WebRTC streamer
webrtc_ctx = webrtc_streamer(
    key="mic",
    mode=WebRtcMode.SENDONLY,
    audio_frame_callback=audio_frame_callback,
    media_stream_constraints={"video": False, "audio": True},
    rtc_configuration=rtc_configuration,
)
```

### Key Considerations

- **Browser Compatibility**: Tested on Chrome 90+, Firefox 88+, Safari 14+, Edge 90+
- **Microphone Permission**: Browser prompts user on first access; denial stops streaming gracefully
- **Sample Rate**: WebRTC defaults to 48000Hz; API supports common rates (16000Hz, 24000Hz, 48000Hz)
- **Frame Size**: Varies by browser (typically 10-20ms chunks); requires buffering for 2-second segments
- **ICE Servers**: STUN server needed for NAT traversal even in SENDONLY mode

### Error Handling for Microphone Permission Denial

```python
# Check WebRTC state
if webrtc_ctx.state.playing:
    st.success("‚úÖ È∫•ÂÖãÈ¢®Â∑≤ÈÄ£Á∑ö")
elif webrtc_ctx.state.signalling:
    st.info("‚è≥ Ê≠£Âú®Âª∫Á´ãÈÄ£Á∑ö...")
else:
    st.warning("‚ö†Ô∏è Ë´ãÂÖÅË®±ÁÄèË¶ΩÂô®Â≠òÂèñÈ∫•ÂÖãÈ¢®‰ª•ÈñãÂßãËΩâÈåÑ")
```

**Error Scenarios**:
- User denies permission: `webrtc_ctx.state.playing` remains False
- Microphone unavailable: Browser shows native error dialog
- Network issues: Connection timeout after 30 seconds

---

## R2: OpenAI Audio Transcription API

### Decision

**Use OpenAI `/audio/transcriptions` endpoint with `gpt-4o-mini-transcribe` model for chunk-based transcription**

### Rationale

1. **Cost Effective**: gpt-4o-mini-transcribe is optimized for transcription at lower cost than gpt-4
2. **High Accuracy**: Latest model with improved accuracy for multiple languages
3. **Simple API**: Single POST request with audio file, no streaming protocol complexity
4. **Auto Language Detection**: No need to specify language; model detects automatically
5. **Production Ready**: Officially supported API endpoint with SLA guarantees

### Alternatives Considered

**OpenAI Realtime API (gpt-realtime-mini)**:
- ‚ùå More expensive per minute
- ‚ùå Requires WebSocket connection (complex integration)
- ‚ùå Overkill for unidirectional transcription
- ‚úÖ Lower latency (<1 second)
- ‚úÖ Supports bidirectional voice conversation
- ‚úÖ Better for interactive use cases

**Whisper API (whisper-1)**:
- ‚ùå Older model, lower accuracy
- ‚ùå Slower processing
- ‚úÖ Lower cost
- ‚úÖ Still supported

**Third-party services (Google Speech-to-Text, Azure Speech)**:
- ‚ùå Additional vendor dependency
- ‚ùå Different API authentication
- ‚ùå Requires separate billing account
- ‚úÖ Potentially better language support
- ‚úÖ Real-time streaming options

### Implementation Pattern

```python
from openai import OpenAI
import os

def transcribe_wav_bytes(wav_bytes: bytes, language: str = None) -> str:
    """
    Transcribe audio chunk using OpenAI API.

    Args:
        wav_bytes: WAV format audio data
        language: Optional ISO-639-1 language code (e.g., 'zh', 'en')

    Returns:
        Transcribed text string

    Raises:
        openai.APIError: API request failed
        openai.APIConnectionError: Network error
        openai.RateLimitError: Rate limit exceeded
    """
    client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

    try:
        # Create temporary file-like object for API
        from io import BytesIO
        audio_file = BytesIO(wav_bytes)
        audio_file.name = "chunk.wav"

        # Call transcription API
        response = client.audio.transcriptions.create(
            model="gpt-4o-mini-transcribe",
            file=audio_file,
            language=language,  # Optional: omit for auto-detect
            response_format="text"
        )

        return response.strip()

    except Exception as e:
        # Log error and return error marker
        print(f"Transcription error: {e}")
        raise
```

### API Parameters

**Required**:
- `model`: "gpt-4o-mini-transcribe"
- `file`: Audio file in WAV, MP3, M4A, FLAC, etc. (WAV recommended for this use case)

**Optional**:
- `language`: ISO-639-1 code (improves accuracy if known)
- `prompt`: Context to guide transcription (e.g., technical terms)
- `response_format`: "text" (default), "json", "srt", "vtt" (use "text" for simplicity)
- `temperature`: 0-1 (default 0; higher = more creative but less accurate)

### Rate Limits and Retry Strategies

**OpenAI Rate Limits** (as of 2025-01):
- Tier 1 (Free): 3 requests/minute
- Tier 2 ($5+ spend): 50 requests/minute
- Tier 3 ($50+ spend): 500 requests/minute

**Retry Strategy**:
```python
from tenacity import retry, stop_after_attempt, wait_exponential
from openai import RateLimitError, APIConnectionError

@retry(
    stop=stop_after_attempt(3),
    wait=wait_exponential(multiplier=1, min=2, max=10),
    retry=lambda e: isinstance(e, (RateLimitError, APIConnectionError))
)
def transcribe_with_retry(wav_bytes: bytes) -> str:
    """Transcribe with exponential backoff retry."""
    return transcribe_wav_bytes(wav_bytes)
```

**Rate Limiting Prevention**:
- Use Voice Activity Detection to reduce API calls (see R4)
- Configurable chunk duration (longer chunks = fewer API calls)
- Queue-based processing prevents overwhelming API with parallel requests

### Error Handling

```python
from openai import APIError, APIConnectionError, RateLimitError, AuthenticationError

try:
    text = transcribe_wav_bytes(wav_bytes)
except AuthenticationError:
    return "[ERROR] ÁÑ°ÊïàÁöÑ API ÈáëÈë∞ÔºåË´ãÊ™¢Êü•Áí∞Â¢ÉËÆäÊï∏ OPENAI_API_KEY"
except RateLimitError:
    return "[ERROR] API Ë´ãÊ±ÇË∂ÖÈÅéÈôêÂà∂ÔºåË´ãÁ®çÂæåÂÜçË©¶ÊàñÂçáÁ¥öÂ∏≥ËôüÁ≠âÁ¥ö"
except APIConnectionError:
    return "[ERROR] Á∂≤Ë∑ØÈÄ£Á∑öÂ§±ÊïóÔºåË´ãÊ™¢Êü•Á∂≤Ë∑ØÈÄ£Á∑ö"
except APIError as e:
    return f"[ERROR] API ÈåØË™§: {e.message}"
except Exception as e:
    return f"[ERROR] Êú™È†êÊúüÁöÑÈåØË™§: {str(e)}"
```

### Key Considerations

- **File Size Limit**: 25MB per request (not a concern for 2-5 second chunks)
- **Supported Formats**: WAV (PCM16) recommended; API accepts MP3, M4A, FLAC, etc.
- **Language Support**: 50+ languages; auto-detection works well for common languages
- **Context Prompt**: Can improve accuracy for technical terms or domain-specific vocabulary
- **Processing Time**: Typically 1-3 seconds for 2-second audio chunk

---

## R3: Audio Processing

### Decision

**Convert PyAV frames to NumPy arrays, accumulate to configurable chunk size, convert to PCM16 WAV with proper header**

### Rationale

1. **Format Compatibility**: OpenAI API requires standard audio formats; WAV with PCM16 is universally supported
2. **Efficient Processing**: NumPy enables fast array operations for audio manipulation
3. **Standard Pipeline**: PyAV ‚Üí NumPy ‚Üí PCM16 ‚Üí WAV is proven pattern in audio processing
4. **Minimal Dependencies**: Only requires PyAV and NumPy (no specialized audio libraries)
5. **Browser Compatibility**: 48kHz is WebRTC standard; no resampling needed

### Alternatives Considered

**Use pydub for audio conversion**:
- ‚ùå Requires FFmpeg dependency
- ‚ùå Heavier library for simple use case
- ‚ùå Slower processing
- ‚úÖ Simpler API
- ‚úÖ More audio format support

**Send raw PCM without WAV header**:
- ‚ùå API requires file format with header
- ‚ùå Less portable
- ‚úÖ Slightly smaller payload
- ‚úÖ Simpler implementation

**Resample to 16kHz**:
- ‚ùå Quality loss from downsampling
- ‚ùå Additional processing overhead
- ‚ùå Unnecessary (API supports 48kHz)
- ‚úÖ Smaller file size
- ‚úÖ Potentially faster API processing

### Implementation Pattern

**Converting PyAV Frames to NumPy Arrays**:

```python
import numpy as np
import av

def process_audio_frame(frame: av.AudioFrame) -> np.ndarray:
    """
    Convert PyAV AudioFrame to mono int16 NumPy array.

    Args:
        frame: PyAV AudioFrame from WebRTC (typically 48000Hz)

    Returns:
        1D NumPy array of int16 samples
    """
    # Extract PCM data as NumPy array
    # Shape: (channels, samples) or (samples,) for mono
    pcm = frame.to_ndarray()

    # Mix to mono if stereo (simple average)
    if pcm.ndim == 2:
        pcm = pcm.mean(axis=0)

    # Ensure int16 format (may be float32 from PyAV)
    if pcm.dtype != np.int16:
        # Normalize float32 [-1.0, 1.0] to int16 [-32768, 32767]
        if pcm.dtype == np.float32:
            pcm = (pcm * 32767).astype(np.int16)
        else:
            pcm = pcm.astype(np.int16, copy=False)

    return pcm
```

**Audio Chunking Logic**:

```python
class AudioChunker:
    """Accumulate audio frames into configurable chunks."""

    def __init__(self, sample_rate: int = 48000, chunk_secs: float = 2.0):
        """
        Initialize chunker.

        Args:
            sample_rate: Audio sample rate in Hz
            chunk_secs: Target chunk duration in seconds
        """
        self.sample_rate = sample_rate
        self.chunk_size = int(sample_rate * chunk_secs)
        self.buffer = np.array([], dtype=np.int16)

    def push(self, pcm: np.ndarray) -> np.ndarray | None:
        """
        Add PCM samples to buffer.

        Args:
            pcm: 1D array of int16 samples

        Returns:
            Complete chunk if buffer full, else None
        """
        # Append to buffer
        self.buffer = np.concatenate([self.buffer, pcm])

        # Check if chunk complete
        if len(self.buffer) >= self.chunk_size:
            # Extract chunk
            chunk = self.buffer[:self.chunk_size]
            # Keep remainder
            self.buffer = self.buffer[self.chunk_size:]
            return chunk

        return None
```

**PCM16 to WAV Conversion**:

```python
import struct
import io

def pcm16_to_wav_bytes(pcm: np.ndarray, sample_rate: int = 48000) -> bytes:
    """
    Convert PCM16 NumPy array to WAV format bytes.

    Args:
        pcm: 1D array of int16 samples
        sample_rate: Sample rate in Hz

    Returns:
        Complete WAV file as bytes
    """
    # WAV header parameters
    num_channels = 1  # Mono
    sample_width = 2  # 16-bit = 2 bytes
    num_frames = len(pcm)

    # Calculate sizes
    data_size = num_frames * num_channels * sample_width
    chunk_size = 36 + data_size

    # Build WAV header (44 bytes)
    wav_header = struct.pack(
        '<4sI4s4sIHHIIHH4sI',
        b'RIFF',           # ChunkID
        chunk_size,       # ChunkSize
        b'WAVE',          # Format
        b'fmt ',          # Subchunk1ID
        16,               # Subchunk1Size (PCM = 16)
        1,                # AudioFormat (1 = PCM)
        num_channels,     # NumChannels
        sample_rate,      # SampleRate
        sample_rate * num_channels * sample_width,  # ByteRate
        num_channels * sample_width,  # BlockAlign
        sample_width * 8,  # BitsPerSample
        b'data',          # Subchunk2ID
        data_size         # Subchunk2Size
    )

    # Combine header and PCM data
    wav_bytes = wav_header + pcm.tobytes()

    return wav_bytes
```

**Complete Audio Pipeline**:

```python
# Initialize chunker
chunker = AudioChunker(sample_rate=48000, chunk_secs=2.0)

def audio_frame_callback(frame: av.AudioFrame) -> av.AudioFrame:
    """Process audio frames and generate WAV chunks."""
    # Convert frame to PCM
    pcm = process_audio_frame(frame)

    # Accumulate samples
    chunk = chunker.push(pcm)

    # If chunk complete, convert to WAV
    if chunk is not None:
        wav_bytes = pcm16_to_wav_bytes(chunk, sample_rate=48000)
        # Send to transcription queue
        audio_queue.put(wav_bytes)

    return frame
```

### Sample Rate Handling

**WebRTC Standard**: 48000Hz (48kHz)
- High quality audio suitable for transcription
- No resampling needed
- Supported by OpenAI API

**Alternative Sample Rates**:
- 16000Hz (16kHz): Common for telephony; lower quality but smaller files
- 24000Hz (24kHz): Mid-range quality
- 44100Hz (44.1kHz): CD quality

**Recommendation**: Use 48kHz (WebRTC default) without resampling to preserve quality

### Key Considerations

- **Byte Order**: WAV format uses little-endian (`<` in struct.pack)
- **Memory Efficiency**: NumPy array operations avoid copying data when possible
- **Buffer Management**: Chunker maintains remainder samples between chunks
- **Thread Safety**: Frame callback runs on WebRTC thread; use queue for cross-thread communication
- **Mono vs Stereo**: Mix to mono to reduce file size (transcription doesn't benefit from stereo)

---

## R4: Voice Activity Detection (VAD)

### Decision

**Use simple RMS-based threshold with configurable sensitivity for cost optimization**

### Rationale

1. **Simplicity**: RMS calculation is straightforward and fast
2. **Sufficient Accuracy**: Effective for filtering silence and background noise
3. **No Dependencies**: No specialized VAD libraries required
4. **Configurable**: Users can adjust threshold based on their environment
5. **Cost Effective**: Reduces API calls by 30-60% in typical usage

### Alternatives Considered

**WebRTC VAD (python-webrtcvad)**:
- ‚ùå Additional dependency
- ‚ùå Fixed to 8kHz, 16kHz, 32kHz (requires resampling from 48kHz)
- ‚ùå Less flexible threshold tuning
- ‚úÖ More sophisticated algorithm
- ‚úÖ Better accuracy for speech detection

**ML-based VAD (Silero VAD)**:
- ‚ùå Requires PyTorch or ONNX runtime (heavy dependencies)
- ‚ùå Model loading overhead
- ‚ùå Overkill for simple use case
- ‚úÖ State-of-the-art accuracy
- ‚úÖ Robust to noise

**Zero-crossing rate (ZCR)**:
- ‚ùå Less reliable for speech vs noise distinction
- ‚ùå Susceptible to low-frequency noise
- ‚úÖ Very simple implementation
- ‚úÖ No computational overhead

### Implementation Pattern

**RMS-based Threshold Calculation**:

```python
import numpy as np

def calculate_rms(pcm: np.ndarray) -> float:
    """
    Calculate Root Mean Square (RMS) of audio signal.

    Args:
        pcm: 1D array of int16 samples

    Returns:
        RMS value (0-32767 range for int16)
    """
    # Convert to float to prevent overflow
    pcm_float = pcm.astype(np.float64)

    # Calculate RMS: sqrt(mean(squares))
    rms = np.sqrt(np.mean(pcm_float ** 2))

    return rms


def has_voice_activity(pcm: np.ndarray, threshold: float = 200.0) -> bool:
    """
    Detect voice activity in audio chunk.

    Args:
        pcm: 1D array of int16 samples
        threshold: RMS threshold for voice detection (50-1000)

    Returns:
        True if voice detected, False otherwise
    """
    rms = calculate_rms(pcm)
    return rms >= threshold
```

**Integration with Audio Chunker**:

```python
class Chunker:
    """Audio chunker with built-in VAD."""

    def __init__(self, sample_rate: int = 48000, chunk_secs: float = 2.0, vad_rms: int = 200):
        self.sample_rate = sample_rate
        self.chunk_size = int(sample_rate * chunk_secs)
        self.vad_threshold = vad_rms
        self.buffer = np.array([], dtype=np.int16)

    def push(self, pcm: np.ndarray) -> np.ndarray | None:
        """
        Add samples and return chunk if complete and contains voice.

        Args:
            pcm: 1D array of int16 samples

        Returns:
            Complete chunk if voice detected, else None
        """
        self.buffer = np.concatenate([self.buffer, pcm])

        if len(self.buffer) >= self.chunk_size:
            chunk = self.buffer[:self.chunk_size]
            self.buffer = self.buffer[self.chunk_size:]

            # Apply VAD filter
            if has_voice_activity(chunk, self.vad_threshold):
                return chunk
            else:
                # Silent chunk - discard
                return None

        return None
```

### Optimal Threshold Ranges

**Recommended Thresholds** (for 48kHz int16 PCM):

| Environment | Threshold | Description |
|-------------|-----------|-------------|
| **Quiet office** | 150-250 | Low background noise |
| **Normal room** | 200-400 | Moderate ambient noise |
| **Noisy environment** | 400-800 | High background noise, AC, traffic |
| **Very noisy** | 800-1200 | Crowded spaces, machinery |

**Calibration Strategy**:
- Start with default 200
- If too many silent chunks transcribed: increase threshold
- If speech is being cut off: decrease threshold
- Provide real-time RMS display for user calibration

**Calibration UI Component**:

```python
import streamlit as st

# Threshold slider
vad_threshold = st.slider(
    "VAD Èü≥ÈáèÈñÄÊ™ª (RMS)",
    min_value=50,
    max_value=1000,
    value=200,
    step=10,
    help="ËºÉÈ´òÁöÑÊï∏ÂÄºÊúÉÈÅéÊøæÊõ¥Â§öÈùúÈü≥ÁâáÊÆµÔºåÈôç‰Ωé API ÊàêÊú¨"
)

# Real-time RMS display (in callback)
if 'current_rms' in st.session_state:
    rms = st.session_state.current_rms
    st.metric("ÁõÆÂâçÈü≥Èáè (RMS)", f"{rms:.0f}")

    # Visual indicator
    if rms >= vad_threshold:
        st.success("üé§ ÂÅµÊ∏¨Âà∞Ë™ûÈü≥")
    else:
        st.info("üîá ÈùúÈü≥‰∏≠")
```

### Cost vs Accuracy Tradeoffs

**Cost Savings**:
- Without VAD: 30 chunks/minute at 2s interval = $0.XX per minute
- With VAD (40% silence filtered): 18 chunks/minute = 40% cost reduction
- Typical savings: 30-60% depending on speaking patterns

**Accuracy Considerations**:
- RMS threshold too high: Cuts off soft speech, incomplete sentences
- RMS threshold too low: Wastes API calls on silence, background noise
- Recommended: Start conservative (lower threshold) and increase based on user feedback

**False Positive/Negative Rates** (RMS @ threshold 200):
- False Positive (silence transcribed): ~10-20% of chunks
- False Negative (speech dropped): <5% for normal speech volume
- Acceptable tradeoff for cost savings

### Key Considerations

- **Threshold Unit**: RMS for int16 ranges from 0 to 32767 (max value)
- **Environmental Adaptation**: Users should adjust threshold based on their setup
- **Speech Patterns**: Pauses between words won't trigger VAD (intentional)
- **Energy Normalization**: Consider normalizing RMS by chunk length if variable durations
- **Fallback**: If VAD is too aggressive, users can set threshold to 0 to disable

---

## R5: Thread Safety

### Decision

**Use queue-based architecture for audio processing with threading.Lock for file writes**

### Rationale

1. **Decoupling**: Audio callback on WebRTC thread, transcription on worker thread
2. **Non-blocking**: Audio callback must return quickly to prevent frame drops
3. **Standard Library**: Python's queue.Queue is thread-safe without additional libraries
4. **File Safety**: threading.Lock prevents concurrent write corruption
5. **Proven Pattern**: Matches existing project patterns (storage_service.py)

### Alternatives Considered

**asyncio with async queues**:
- ‚ùå Streamlit not designed for async (experimental support only)
- ‚ùå More complex integration with streamlit-webrtc
- ‚ùå Mixing threading and asyncio is error-prone
- ‚úÖ Better for I/O-bound operations
- ‚úÖ More scalable for high concurrency

**Direct file locking with fcntl/msvcrt**:
- ‚ùå Project already uses this in storage_service.py for JSON files
- ‚ùå Overkill for append-only writes from single worker
- ‚úÖ Protects against external process writes
- ‚úÖ More robust cross-process locking

**No thread safety (single-threaded)**:
- ‚ùå Audio callback would block on API calls (frame drops)
- ‚ùå UI would freeze during transcription
- ‚ùå Unacceptable user experience
- ‚úÖ Simpler implementation
- ‚úÖ No race conditions

### Implementation Pattern

**Queue-based Architecture**:

```python
import queue
import threading
import time

# Initialize audio queue (bounded to prevent memory overflow)
audio_queue = queue.Queue(maxsize=32)

def audio_frame_callback(frame: av.AudioFrame) -> av.AudioFrame:
    """
    WebRTC callback - runs on separate thread.
    Must return quickly to prevent frame drops.
    """
    # Process frame to PCM
    pcm = process_audio_frame(frame)

    # Accumulate to chunk
    chunk = chunker.push(pcm)

    if chunk is not None:
        wav_bytes = pcm16_to_wav_bytes(chunk, sample_rate=48000)

        try:
            # Non-blocking put with timeout
            audio_queue.put_nowait(wav_bytes)
        except queue.Full:
            # Drop frame if queue full (prevent blocking)
            pass

    return frame


def transcription_worker():
    """
    Background worker thread for processing audio queue.
    Runs continuously until application shutdown.
    """
    while True:
        # Blocking get - waits for audio chunks
        wav_bytes = audio_queue.get()

        try:
            # Call OpenAI API
            text = transcribe_wav_bytes(wav_bytes)
            text = text.strip()

            if text:
                # Append to session state (thread-safe in Streamlit)
                st.session_state.transcript.append(text)

                # Write to file
                if st.session_state.file_path:
                    with st.session_state.file_lock:
                        with open(st.session_state.file_path, "a", encoding="utf-8") as f:
                            f.write(text + "\n")

        except Exception as e:
            # Log error and continue processing
            error_msg = f"[ERROR] {str(e)}"
            st.session_state.transcript.append(error_msg)

        finally:
            # Mark task complete
            audio_queue.task_done()
            # Small delay to prevent tight loop
            time.sleep(0.01)


# Start worker thread on application init
if "worker_started" not in st.session_state:
    threading.Thread(target=transcription_worker, daemon=True).start()
    st.session_state.worker_started = True
```

**File Locking for Concurrent Writes**:

```python
import threading

# Initialize file lock in session state
if "file_lock" not in st.session_state:
    st.session_state.file_lock = threading.Lock()

# Use lock for all file operations
def append_to_transcript(file_path: str, text: str):
    """Thread-safe file append."""
    with st.session_state.file_lock:
        with open(file_path, "a", encoding="utf-8") as f:
            f.write(text + "\n")
```

### Streamlit Session State Thread Safety

**Key Points**:
- Streamlit session state is NOT thread-safe by default
- Reads from session state are safe (reference counting)
- Writes to session state require synchronization if from multiple threads
- Our pattern: Only worker thread writes to session state (safe)

**Safe Pattern** (single writer):
```python
# Worker thread writes
st.session_state.transcript.append(text)

# Main thread reads
joined_text = "\n".join(st.session_state.transcript)
```

**Unsafe Pattern** (multiple writers - avoid):
```python
# Thread A
st.session_state.counter += 1  # Race condition

# Thread B
st.session_state.counter += 1  # Race condition
```

**If Multiple Writers Needed** (use lock):
```python
if "state_lock" not in st.session_state:
    st.session_state.state_lock = threading.Lock()

with st.session_state.state_lock:
    st.session_state.counter += 1
```

### Queue Size Considerations

**Bounded Queue** (`maxsize=32`):
- **Pros**: Prevents memory overflow if transcription slower than audio capture
- **Cons**: May drop audio chunks if queue full
- **Recommendation**: 32 chunks = ~64 seconds of audio buffered (sufficient)

**Unbounded Queue** (`maxsize=0`):
- **Pros**: Never drops audio chunks
- **Cons**: Memory grows unbounded if API slow or down
- **Recommendation**: Avoid; use bounded with monitoring

**Backpressure Strategy**:
```python
try:
    audio_queue.put_nowait(wav_bytes)
except queue.Full:
    # Log dropped chunk
    if "dropped_chunks" not in st.session_state:
        st.session_state.dropped_chunks = 0
    st.session_state.dropped_chunks += 1
```

### Key Considerations

- **Daemon Threads**: Worker thread should be daemon so it terminates with main process
- **Graceful Shutdown**: Not critical for web app (browser close terminates process)
- **Error Isolation**: Try-except in worker prevents one error from stopping all processing
- **File Buffering**: Use default buffering for append mode (automatic flush on newline)
- **Cross-User Isolation**: Each Streamlit session has independent session_state (no leakage)

### Comparison with Existing Patterns

**Existing Project Pattern** (storage_service.py):
- Uses file locking with fcntl/msvcrt for JSON files
- Atomic writes with temp file + rename
- Suitable for read-modify-write workflows

**Transcription Pattern** (this feature):
- Uses threading.Lock for simplicity (single process, append-only)
- Direct append without temp file (streaming nature)
- Suitable for append-only workflows

**Consistency**: Both use context manager pattern (`with lock:`) for safety

---

## R6: Testing Strategy

### Decision

**Use pytest with mock objects for WebRTC/API, integration tests for audio pipeline, manual testing for UI**

### Rationale

1. **Existing Framework**: Project already uses pytest with good coverage
2. **Mockability**: OpenAI SDK and WebRTC can be mocked effectively
3. **Fast Tests**: Mocking avoids API costs and real microphone dependency
4. **Comprehensive**: Covers unit, integration, and manual testing layers
5. **CI/CD Ready**: All automated tests run without external dependencies

### Alternatives Considered

**Real API calls in tests**:
- ‚ùå Costly (API charges for test runs)
- ‚ùå Slow (network latency)
- ‚ùå Flaky (network failures, rate limits)
- ‚ùå Requires API key in CI environment
- ‚úÖ Tests real API behavior
- ‚úÖ No mock discrepancies

**Recorded audio files for testing**:
- ‚ùå Increases repository size
- ‚ùå Still requires API mocking for unit tests
- ‚úÖ Realistic audio data
- ‚úÖ Useful for integration tests

**Selenium/Playwright for UI tests**:
- ‚ùå Heavy dependencies
- ‚ùå Complex WebRTC simulation
- ‚ùå Slow execution
- ‚úÖ Full end-to-end testing
- ‚úÖ Tests actual browser behavior

### Implementation Pattern

**Mocking WebRTC Audio Input**:

```python
import pytest
import numpy as np
import av

@pytest.fixture
def mock_audio_frame():
    """Create mock PyAV AudioFrame for testing."""
    # Generate 10ms of silent audio at 48kHz
    sample_rate = 48000
    duration_ms = 10
    num_samples = int(sample_rate * duration_ms / 1000)

    # Create silent PCM data
    pcm = np.zeros(num_samples, dtype=np.int16)

    # Create AudioFrame
    frame = av.AudioFrame.from_ndarray(
        pcm.reshape(1, -1),  # Shape: (channels, samples)
        format='s16',
        layout='mono'
    )
    frame.sample_rate = sample_rate

    return frame


def test_audio_frame_callback(mock_audio_frame):
    """Test audio frame processing."""
    # Process frame
    result = audio_frame_callback(mock_audio_frame)

    # Verify frame returned unchanged
    assert result == mock_audio_frame

    # Verify frame added to chunker buffer
    # ... additional assertions
```

**Mocking OpenAI API**:

```python
import pytest
from unittest.mock import patch, MagicMock

@pytest.fixture
def mock_openai_client():
    """Mock OpenAI client for testing."""
    with patch('openai.OpenAI') as mock_client:
        # Configure mock response
        mock_instance = MagicMock()
        mock_instance.audio.transcriptions.create.return_value = "Ê∏¨Ë©¶ËΩâÈåÑÊñáÂ≠ó"
        mock_client.return_value = mock_instance

        yield mock_instance


def test_transcribe_wav_bytes(mock_openai_client):
    """Test WAV transcription with mocked API."""
    # Create sample WAV bytes
    pcm = np.zeros(96000, dtype=np.int16)  # 2 seconds at 48kHz
    wav_bytes = pcm16_to_wav_bytes(pcm, sample_rate=48000)

    # Call transcription
    result = transcribe_wav_bytes(wav_bytes)

    # Verify API called correctly
    mock_openai_client.audio.transcriptions.create.assert_called_once()
    call_args = mock_openai_client.audio.transcriptions.create.call_args
    assert call_args.kwargs['model'] == 'gpt-4o-mini-transcribe'

    # Verify result
    assert result == "Ê∏¨Ë©¶ËΩâÈåÑÊñáÂ≠ó"


def test_transcribe_api_error(mock_openai_client):
    """Test API error handling."""
    from openai import APIError

    # Configure mock to raise error
    mock_openai_client.audio.transcriptions.create.side_effect = APIError("Test error")

    # Verify error handling
    with pytest.raises(APIError):
        transcribe_wav_bytes(b'fake_wav_data')
```

**Integration Testing Approach**:

```python
def test_audio_pipeline_integration():
    """
    Test complete audio processing pipeline without real API.
    Uses mock API but real audio processing logic.
    """
    # Setup
    chunker = Chunker(sample_rate=48000, chunk_secs=2.0, vad_rms=200)

    # Generate 5 seconds of audio (voice simulation)
    sample_rate = 48000
    duration = 5.0
    num_samples = int(sample_rate * duration)

    # Create sine wave (simulates voice frequency)
    t = np.linspace(0, duration, num_samples)
    frequency = 300  # Hz (typical speech)
    pcm = (np.sin(2 * np.pi * frequency * t) * 10000).astype(np.int16)

    # Process in frame-sized chunks (10ms)
    frame_size = int(sample_rate * 0.01)
    chunks_generated = []

    for i in range(0, len(pcm), frame_size):
        frame_pcm = pcm[i:i+frame_size]
        chunk = chunker.push(frame_pcm)
        if chunk is not None:
            chunks_generated.append(chunk)

    # Verify chunks generated
    assert len(chunks_generated) == 2  # 5 seconds / 2 second chunks = 2.5 -> 2 complete
    assert len(chunks_generated[0]) == 96000  # 2 seconds * 48000 Hz

    # Verify WAV conversion
    wav_bytes = pcm16_to_wav_bytes(chunks_generated[0], sample_rate=48000)
    assert len(wav_bytes) == 44 + (96000 * 2)  # Header + PCM data
    assert wav_bytes[:4] == b'RIFF'  # Verify WAV header


def test_vad_filtering():
    """Test voice activity detection filtering."""
    chunker = Chunker(sample_rate=48000, chunk_secs=2.0, vad_rms=200)

    # Generate silent chunk
    silent_pcm = np.zeros(96000, dtype=np.int16)

    # Push silent chunk
    result = chunker.push(silent_pcm)

    # Verify silent chunk filtered out
    assert result is None

    # Generate loud chunk
    loud_pcm = (np.random.randn(96000) * 5000).astype(np.int16)

    # Push loud chunk
    result = chunker.push(loud_pcm)

    # Verify loud chunk passed through
    assert result is not None
    assert len(result) == 96000
```

**Performance Benchmarking**:

```python
import time
import pytest

def test_transcription_latency(mock_openai_client):
    """
    Verify transcription meets latency requirements.
    Success criteria: First text appears within 5 seconds.
    """
    # Mock API with realistic delay
    def delayed_response(*args, **kwargs):
        time.sleep(1.5)  # Simulate API latency
        return "Ê∏¨Ë©¶ÁµêÊûú"

    mock_openai_client.audio.transcriptions.create.side_effect = delayed_response

    # Generate 2-second audio chunk
    pcm = np.zeros(96000, dtype=np.int16)
    wav_bytes = pcm16_to_wav_bytes(pcm, sample_rate=48000)

    # Measure transcription time
    start = time.time()
    result = transcribe_wav_bytes(wav_bytes)
    elapsed = time.time() - start

    # Verify latency requirement (< 3 seconds for transcription)
    assert elapsed < 3.0
    assert result == "Ê∏¨Ë©¶ÁµêÊûú"


@pytest.mark.slow
def test_sustained_transcription():
    """
    Test 100 chunks without memory leaks or errors.
    Success criteria: Handle 100+ chunks (3-8 minutes of speech).
    """
    chunker = Chunker(sample_rate=48000, chunk_secs=2.0, vad_rms=200)

    chunks_processed = 0
    for _ in range(100):
        # Generate voice-like audio
        pcm = (np.random.randn(96000) * 5000).astype(np.int16)
        chunk = chunker.push(pcm)

        if chunk is not None:
            # Convert to WAV (memory test)
            wav_bytes = pcm16_to_wav_bytes(chunk, sample_rate=48000)
            assert len(wav_bytes) > 0
            chunks_processed += 1

    # Verify processing capacity
    assert chunks_processed >= 100
```

**Manual Testing Checklist**:

```python
"""
Manual Test Plan: Real-time Microphone Transcription
====================================================

Prerequisites:
- OpenAI API key configured in .env
- Physical microphone connected
- Modern web browser (Chrome 90+, Firefox 88+, Safari 14+)

Test Cases:
-----------

1. Microphone Permission
   [ ] Click "ÈñãÂßãËΩâÈåÑ" button
   [ ] Verify browser shows permission prompt
   [ ] Grant permission
   [ ] Verify "È∫•ÂÖãÈ¢®Â∑≤ÈÄ£Á∑ö" message appears
   [ ] Deny permission (separate test)
   [ ] Verify "Ë´ãÂÖÅË®±ÁÄèË¶ΩÂô®Â≠òÂèñÈ∫•ÂÖãÈ¢®" warning appears

2. Real-time Transcription
   [ ] Speak clearly in English for 5 seconds
   [ ] Verify text appears within 5 seconds of starting
   [ ] Verify text accuracy >80%
   [ ] Speak in Chinese for 5 seconds
   [ ] Verify Chinese text appears correctly
   [ ] Verify each 2-second chunk generates new text

3. File Persistence
   [ ] Start transcription without custom filename
   [ ] Verify file created in resource/ with timestamp name
   [ ] Speak for 10 seconds
   [ ] Stop transcription
   [ ] Check file contains all transcribed text
   [ ] Verify start/end timestamps in file

4. VAD Testing
   [ ] Set VAD threshold to 200
   [ ] Remain silent for 10 seconds
   [ ] Verify no text generated (or minimal error messages)
   [ ] Speak normally
   [ ] Verify text appears
   [ ] Set VAD threshold to 50
   [ ] Speak softly
   [ ] Verify soft speech captured

5. Error Handling
   [ ] Disconnect internet after starting
   [ ] Speak during disconnection
   [ ] Verify [ERROR] messages appear
   [ ] Reconnect internet
   [ ] Verify transcription resumes
   [ ] Check file still contains earlier content

6. Browser Compatibility
   [ ] Test on Chrome
   [ ] Test on Firefox
   [ ] Test on Safari
   [ ] Test on Edge
   [ ] Verify consistent behavior across browsers

7. Performance
   [ ] Transcribe continuously for 5 minutes
   [ ] Verify no memory leaks (check browser task manager)
   [ ] Verify UI remains responsive
   [ ] Verify all chunks processed (check file)
"""
```

### Testing Layers

| Layer | Coverage | Tools | Speed | Purpose |
|-------|----------|-------|-------|---------|
| **Unit Tests** | Individual functions | pytest, mock | Fast (<1s) | Verify logic correctness |
| **Integration Tests** | Audio pipeline | pytest, NumPy | Medium (~10s) | Verify component interaction |
| **Manual Tests** | Full UI + API | Browser, microphone | Slow (minutes) | Verify user experience |
| **Performance Tests** | Latency, memory | pytest, time | Slow (~1min) | Verify success criteria |

### Key Considerations

- **API Cost Control**: Never use real API in automated tests
- **Mock Accuracy**: Ensure mocks match real API behavior (response format, errors)
- **Audio Generation**: Use NumPy to generate test audio (sine waves for voice simulation)
- **Flaky Tests**: Avoid timing dependencies in unit tests; use mocks for delays
- **CI/CD Integration**: All automated tests must run without environment variables (except manual tests)
- **Test Data**: Keep test audio short (<1 second) to minimize test execution time
- **Coverage Target**: Aim for >80% code coverage on audio processing logic

---

## Final Recommendations

### Implementation Checklist

1. ‚úÖ **Use streamlit-webrtc** with SENDONLY mode for microphone access
2. ‚úÖ **Implement audio_frame_callback** with non-blocking queue.put_nowait()
3. ‚úÖ **Use OpenAI gpt-4o-mini-transcribe** via /audio/transcriptions endpoint
4. ‚úÖ **Convert audio pipeline**: PyAV ‚Üí NumPy ‚Üí PCM16 ‚Üí WAV with 44-byte header
5. ‚úÖ **Implement RMS-based VAD** with configurable threshold (default 200)
6. ‚úÖ **Use queue-based architecture** with daemon worker thread
7. ‚úÖ **Protect file writes** with threading.Lock in session state
8. ‚úÖ **Write comprehensive tests** with mocked API and WebRTC components

### Performance Targets

- ‚úÖ First transcribed text appears within 5 seconds of speaking (SC-001)
- ‚úÖ Transcribed text appears within 3 seconds of completing each chunk (SC-002)
- ‚úÖ File writes complete within 1 second of transcription (SC-003)
- ‚úÖ Support 30-minute transcription sessions without crashes (SC-004)
- ‚úÖ Handle at least 100 audio chunks per session (SC-005)

### Architecture Decisions

| Component | Technology | Rationale |
|-----------|-----------|-----------|
| **Microphone Access** | streamlit-webrtc | Native Streamlit integration, WebRTC standard |
| **Transcription API** | gpt-4o-mini-transcribe | Cost-effective, high accuracy, simple API |
| **Audio Processing** | PyAV + NumPy | Efficient, minimal dependencies |
| **VAD Algorithm** | RMS threshold | Simple, configurable, sufficient accuracy |
| **Concurrency** | queue.Queue + threading | Decouples capture from processing, standard library |
| **File Locking** | threading.Lock | Simple, sufficient for single-process append |
| **Testing** | pytest + mock | Fast, cost-free, comprehensive coverage |

### Risk Mitigation

| Risk | Mitigation |
|------|-----------|
| **API rate limits** | VAD filtering, configurable chunk duration, retry with backoff |
| **Frame drops** | Non-blocking queue operations, bounded queue size |
| **File corruption** | Threading lock, UTF-8 encoding, append mode |
| **Memory leaks** | Daemon threads, bounded queue, no PIL image objects |
| **Browser incompatibility** | Test on major browsers, graceful WebRTC state handling |
| **Network failures** | Try-except error handling, continue processing after errors |

### Integration Points

**Existing Patterns to Follow**:
1. File operations: Use context managers (similar to storage_service.py)
2. Error handling: Display user-friendly messages (similar to dashboard.py)
3. UI styling: Match existing dark theme gradient
4. Testing: Follow pytest structure in tests/ directory
5. Documentation: Maintain CLAUDE.md format and detail level

**New Patterns Introduced**:
1. Real-time audio streaming (streamlit-webrtc callback pattern)
2. Background worker thread (daemon thread with queue processing)
3. Append-only file writes (different from atomic JSON writes)
4. External API integration (OpenAI SDK usage pattern)

---

## References

- **streamlit-webrtc Documentation**: [GitHub](https://github.com/whitphx/streamlit-webrtc)
- **OpenAI Audio API**: [Speech to Text Guide](https://platform.openai.com/docs/guides/speech-to-text)
- **PyAV Documentation**: [PyAV Docs](https://pyav.org/)
- **NumPy Audio Processing**: [NumPy Manual](https://numpy.org/doc/stable/)
- **WAV File Format**: [WAVE PCM soundfile format](http://soundfile.sapp.org/doc/WaveFormat/)
- **WebRTC Standards**: [W3C Media Capture](https://www.w3.org/TR/mediacapture-streams/)
- **Python Threading**: [threading module](https://docs.python.org/3/library/threading.html)
- **Project Constitution**: `/CLAUDE.md` (project coding standards)
- **PR Requirements**: `/requirements/PR-gpt4o-transcripbe.md` (implementation reference)

---

## Appendix: Dependency Matrix

### New Dependencies Required

```python
# requirements.txt additions
streamlit-webrtc==0.47.1      # WebRTC microphone access in Streamlit
av==11.0.0                    # PyAV for audio frame processing
numpy==1.24.3                 # Audio array manipulation (may already exist)
openai==1.12.0                # OpenAI API client
python-dotenv==1.0.0          # Environment variable management (may already exist)
```

### Dependency Analysis

| Package | Size | Purpose | Alternatives Considered |
|---------|------|---------|------------------------|
| streamlit-webrtc | ~50KB | Browser microphone access | Custom JS component (too complex) |
| av (PyAV) | ~5MB | Audio frame decoding | soundfile (lacks frame-level control) |
| numpy | ~15MB | Array operations | Built-in arrays (too slow) |
| openai | ~200KB | API client | requests (manual API handling) |
| python-dotenv | ~20KB | Environment config | os.getenv only (less convenient) |

**Total Additional Size**: ~20MB (numpy likely already in environment)

### Compatibility Matrix

| Python Version | streamlit-webrtc | PyAV | OpenAI SDK |
|----------------|------------------|------|------------|
| 3.9.6 (project) | ‚úÖ Supported | ‚úÖ Supported | ‚úÖ Supported |
| 3.10+ | ‚úÖ Supported | ‚úÖ Supported | ‚úÖ Supported |
| 3.8 | ‚ö†Ô∏è Limited | ‚úÖ Supported | ‚úÖ Supported |

**Recommendation**: Continue using Python 3.9.6 (current project version)

---

**Research Complete** ‚úÖ
All technical decisions finalized and ready for implementation Phase 1 (Design Artifacts).
