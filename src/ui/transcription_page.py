"""
Real-time microphone voice transcription UI page.

Uses WebRTC for audio capture and Whisper API for transcription:
- Single WAV file for entire recording session
- Background chunking and transcription (every 3 seconds)
- Accumulated transcript display with minimal UI updates
"""

import os
import queue
import threading
import time
import wave
import io
from datetime import datetime
from pathlib import Path
from typing import Optional

import av
import numpy as np
import streamlit as st
from streamlit_webrtc import RTCConfiguration, WebRtcMode, webrtc_streamer

from src.services.audio_service import process_audio_frame
from src.utils.audio_utils import calculate_rms

SAMPLE_RATE = 48000
SAMPLE_WIDTH = 2  # bytes (int16)
ICE_SERVERS = [{"urls": ["stun:stun.l.google.com:19302"]}]
AUDIO_GAIN = 2.0  # Volume boost multiplier
TRANSCRIPTION_CHUNK_DURATION = 3.0  # Transcribe every 3 seconds

# Global state
_audio_queue: queue.Queue = queue.Queue(maxsize=128)
_transcription_queue: queue.Queue = queue.Queue(maxsize=32)
_recording_active = False
_recording_lock = threading.Lock()
_wav_writer: Optional[wave.Wave_write] = None
_wav_path: Optional[Path] = None
_wav_lock = threading.Lock()
_bytes_written = 0
_last_rms = 0.0
_rms_lock = threading.Lock()
_audio_worker_thread: Optional[threading.Thread] = None
_audio_worker_stop = threading.Event()
_transcription_worker_thread: Optional[threading.Thread] = None
_transcription_worker_stop = threading.Event()

# Transcript accumulation
_transcript_segments = []  # List of transcript strings
_transcript_lock = threading.Lock()

# Audio buffer for transcription
_transcription_buffer = []  # List of numpy arrays
_transcription_buffer_lock = threading.Lock()
_last_transcription_time = 0


def render_transcription_page() -> None:
    """Render real-time voice transcription page."""
    st.title("ğŸ¤ å³æ™‚èªéŸ³è½‰éŒ„ï¼ˆWhisper APIï¼‰")
    st.caption("ä½¿ç”¨ WebRTC éŒ„éŸ³ä¸¦é€é Whisper API èƒŒæ™¯è½‰éŒ„ç‚ºé€å­—ç¨¿")

    _initialize_session_state()
    _check_api_key()

    if not st.session_state.api_key_set:
        _render_api_key_input()
        return

    _render_controls()
    _render_webrtc_stream()
    _render_status()
    _render_transcript_display()


def _initialize_session_state() -> None:
    """Initialize session state variables."""
    if "api_key_set" not in st.session_state:
        st.session_state.api_key_set = False
    if "recording_active" not in st.session_state:
        st.session_state.recording_active = False
    if "recording_path" not in st.session_state:
        st.session_state.recording_path = ""
    if "recording_start_time" not in st.session_state:
        st.session_state.recording_start_time = None
    if "last_transcript" not in st.session_state:
        st.session_state.last_transcript = ""
    if "last_transcript_path" not in st.session_state:
        st.session_state.last_transcript_path = ""
    if "transcription_status" not in st.session_state:
        st.session_state.transcription_status = ""
    if "mic_ready" not in st.session_state:
        st.session_state.mic_ready = False
    if "total_bytes" not in st.session_state:
        st.session_state.total_bytes = 0
    if "current_rms" not in st.session_state:
        st.session_state.current_rms = 0.0
    if "realtime_transcript" not in st.session_state:
        st.session_state.realtime_transcript = ""
    if "segment_count" not in st.session_state:
        st.session_state.segment_count = 0
    if "last_ui_update" not in st.session_state:
        st.session_state.last_ui_update = 0


def _check_api_key() -> None:
    """Check for API key in environment."""
    from dotenv import load_dotenv
    load_dotenv()

    if os.getenv("OPENAI_API_KEY"):
        st.session_state.api_key_set = True
    elif "OPENAI_API_KEY_MANUAL" in st.session_state:
        if st.session_state.OPENAI_API_KEY_MANUAL:
            os.environ["OPENAI_API_KEY"] = st.session_state.OPENAI_API_KEY_MANUAL
            st.session_state.api_key_set = True


def _render_api_key_input() -> None:
    """Render API key input field."""
    st.warning("âš ï¸ è«‹å…ˆè¨­å®š OpenAI API Key")

    api_key = st.text_input(
        "OpenAI API Key",
        type="password",
        help="è¼¸å…¥æ‚¨çš„ OpenAI API Keyï¼Œæˆ–è¨­å®šåœ¨ .env æª”æ¡ˆä¸­",
        key="api_key_input"
    )

    if api_key:
        st.session_state.OPENAI_API_KEY_MANUAL = api_key
        os.environ["OPENAI_API_KEY"] = api_key
        st.session_state.api_key_set = True
        st.rerun()


def _render_controls() -> None:
    """Render control buttons."""
    st.markdown("#### ğŸ™ï¸ éŒ„éŸ³æ§åˆ¶")

    col1, col2 = st.columns(2)

    with col1:
        can_start = (
            not st.session_state.recording_active and
            st.session_state.api_key_set
        )

        if st.button(
            "â–¶ï¸ é–‹å§‹éŒ„éŸ³",
            type="primary",
            use_container_width=True,
            disabled=not can_start,
            help="é–‹å§‹éŒ„éŸ³å‰è«‹ç¢ºèªéº¥å…‹é¢¨æ¬Šé™å·²æˆäºˆ"
        ):
            if st.session_state.mic_ready:
                _start_recording()
                st.rerun()
            else:
                st.error("âš ï¸ è«‹å…ˆç­‰å¾…éº¥å…‹é¢¨é€£ç·šæˆåŠŸï¼ˆç¶ è‰²è¨Šæ¯ï¼‰ï¼Œç„¶å¾Œå†é»æ“Šé–‹å§‹éŒ„éŸ³")

    with col2:
        if st.button(
            "â¹ï¸ åœæ­¢éŒ„éŸ³",
            type="secondary",
            use_container_width=True,
            disabled=not st.session_state.recording_active
        ):
            _stop_recording()
            st.rerun()

    if st.session_state.recording_active:
        st.info("ğŸ”´ éŒ„éŸ³ä¸­... å³æ™‚è½‰éŒ„çµæœå°‡åœ¨ä¸‹æ–¹é¡¯ç¤º")
    elif not st.session_state.mic_ready:
        st.warning("ğŸ¤ ç­‰å¾…éº¥å…‹é¢¨é€£ç·š... è«‹ç¢ºèªç€è¦½å™¨å·²æˆæ¬Šéº¥å…‹é¢¨æ¬Šé™")
    else:
        st.success("âœ… éº¥å…‹é¢¨å·²å°±ç·’ï¼Œå¯ä»¥é–‹å§‹éŒ„éŸ³")


def _render_webrtc_stream() -> None:
    """Render WebRTC microphone stream."""
    st.markdown("#### ğŸ™ï¸ éº¥å…‹é¢¨ä¸²æµ")

    def audio_callback(frame: av.AudioFrame) -> av.AudioFrame:
        global _last_rms

        # Process audio frame with gain
        pcm_array = process_audio_frame(frame, gain=AUDIO_GAIN)

        # Calculate RMS
        rms = float(calculate_rms(pcm_array))
        with _rms_lock:
            _last_rms = rms

        # Add to queues if recording
        with _recording_lock:
            if _recording_active:
                # Add to WAV writer queue
                try:
                    _audio_queue.put_nowait(pcm_array.tobytes())
                except queue.Full:
                    pass

                # Add to transcription buffer
                with _transcription_buffer_lock:
                    _transcription_buffer.append(pcm_array)

        return frame

    rtc_configuration = RTCConfiguration({"iceServers": ICE_SERVERS})
    webrtc_ctx = webrtc_streamer(
        key="transcription-mic",
        mode=WebRtcMode.SENDONLY,
        audio_frame_callback=audio_callback,
        media_stream_constraints={"audio": True, "video": False},
        rtc_configuration=rtc_configuration,
        async_processing=True,
        desired_playing_state=True,
    )

    # Update mic ready status
    if webrtc_ctx.state.playing:
        st.session_state.mic_ready = True
        st.success("ğŸ§ éº¥å…‹é¢¨å·²é€£ç·šï¼ŒéŸ³è¨Šä¸²æµæ­£å¸¸")
    elif webrtc_ctx.state.signalling:
        st.session_state.mic_ready = False
        st.info("ğŸ”„ æ­£åœ¨å»ºç«‹ WebRTC é€£ç·šï¼Œè«‹ç¨å€™...")
    else:
        st.session_state.mic_ready = False
        st.warning("âš ï¸ éº¥å…‹é¢¨æœªé€£ç·šï¼Œè«‹æª¢æŸ¥ç€è¦½å™¨æ¬Šé™")


def _render_status() -> None:
    """Render recording status."""
    st.markdown("#### ğŸ“Š éŒ„éŸ³ç‹€æ…‹")

    if st.session_state.recording_path:
        st.write(f"ğŸ“ æª”æ¡ˆï¼š`{st.session_state.recording_path}`")
    else:
        st.write("ğŸ“ å°šæœªé–‹å§‹éŒ„éŸ³")

    # Update from global state
    with _wav_lock:
        total_bytes = _bytes_written
    with _rms_lock:
        current_rms = _last_rms

    st.session_state.total_bytes = total_bytes
    st.session_state.current_rms = current_rms

    if total_bytes > 0:
        duration_sec = total_bytes / (SAMPLE_RATE * SAMPLE_WIDTH)
        st.write(f"â±ï¸ å·²éŒ„è£½ï¼š{duration_sec:.1f} ç§’")
    else:
        st.write("â±ï¸ å·²éŒ„è£½ï¼š0.0 ç§’")

    st.write(f"ğŸ”Š ç•¶å‰ RMSï¼š{current_rms:.1f}")
    st.write(f"ğŸšï¸ æ¡æ¨£ç‡ï¼š{SAMPLE_RATE} Hz")
    st.write(f"ğŸ“ˆ éŸ³é‡å¢ç›Šï¼š{AUDIO_GAIN}x")

    if st.session_state.recording_active:
        st.write(f"ğŸ“ å·²è½‰éŒ„æ®µæ•¸ï¼š{st.session_state.segment_count}")

    if st.session_state.recording_start_time:
        elapsed = time.time() - st.session_state.recording_start_time
        st.write(f"ğŸ•’ éŒ„éŸ³æ™‚é•·ï¼š{elapsed:.1f} ç§’")


def _render_transcript_display() -> None:
    """Render transcript display area."""
    st.markdown("#### ğŸ“„ å³æ™‚è½‰éŒ„çµæœ")

    if st.session_state.transcription_status:
        st.info(st.session_state.transcription_status)

    # Show real-time transcript during recording
    if st.session_state.recording_active:
        # Update from global state
        with _transcript_lock:
            current_transcript = "\n".join(_transcript_segments)
            segment_count = len(_transcript_segments)

        st.session_state.realtime_transcript = current_transcript
        st.session_state.segment_count = segment_count

        if current_transcript:
            st.text_area(
                "å³æ™‚é€å­—ç¨¿ï¼ˆéŒ„éŸ³ä¸­...ï¼‰",
                value=current_transcript,
                height=300,
                help="å³æ™‚è½‰éŒ„çš„çµæœï¼ˆæ¯ 3 ç§’æ›´æ–°ï¼‰"
            )
            st.caption(f"ğŸ“Š å·²è½‰éŒ„ï¼š{len(current_transcript)} å­—å…ƒ | åˆ†æ®µæ•¸ï¼š{segment_count}")
        else:
            st.info("ğŸ¤ éŒ„éŸ³ä¸­ï¼Œç­‰å¾…ç¬¬ä¸€æ®µè½‰éŒ„çµæœï¼ˆç´„ 3 ç§’å¾Œï¼‰...")

        # Auto-refresh every 2 seconds to show updates (not too frequent)
        current_time = time.time()
        if current_time - st.session_state.last_ui_update >= 2.0:
            st.session_state.last_ui_update = current_time
            time.sleep(0.1)
            st.rerun()

    # Show final transcript after recording stopped
    elif st.session_state.last_transcript:
        st.text_area(
            "å®Œæ•´é€å­—ç¨¿",
            value=st.session_state.last_transcript,
            height=300,
            help="æœ€çµ‚å®Œæ•´çš„è½‰éŒ„çµæœ"
        )

        st.caption(f"ğŸ“Š å­—æ•¸ï¼š{len(st.session_state.last_transcript)} å­—å…ƒ")

        if st.session_state.last_transcript_path:
            st.caption(f"ğŸ’¾ å·²ä¿å­˜è‡³ï¼š`{st.session_state.last_transcript_path}`")

            # Download button
            st.download_button(
                label="ğŸ“¥ ä¸‹è¼‰é€å­—ç¨¿",
                data=st.session_state.last_transcript.encode("utf-8"),
                file_name=Path(st.session_state.last_transcript_path).name,
                mime="text/plain",
                use_container_width=True
            )
    else:
        st.info("é»æ“Šã€Œé–‹å§‹éŒ„éŸ³ã€å¾Œï¼Œå³æ™‚è½‰éŒ„çµæœå°‡é¡¯ç¤ºåœ¨æ­¤è™•")


def _start_recording() -> None:
    """Start recording and transcription."""
    global _recording_active, _wav_writer, _wav_path, _bytes_written
    global _audio_worker_thread, _transcription_worker_thread
    global _transcript_segments, _transcription_buffer, _last_transcription_time

    # Create resource directory
    resource_dir = Path("resource")
    resource_dir.mkdir(parents=True, exist_ok=True)

    # Generate filename
    timestamp = datetime.now().strftime("%Y%m%d-%H%M%S")
    wav_filename = f"recording-{timestamp}.wav"
    wav_path = resource_dir / wav_filename

    # Open WAV file (will write continuously to this single file)
    try:
        wav_writer = wave.open(str(wav_path), "wb")
        wav_writer.setnchannels(1)
        wav_writer.setsampwidth(SAMPLE_WIDTH)
        wav_writer.setframerate(SAMPLE_RATE)
    except Exception as exc:
        st.error(f"ç„¡æ³•å»ºç«‹éŸ³è¨Šæª”æ¡ˆï¼š{exc}")
        return

    with _wav_lock:
        _wav_writer = wav_writer
        _wav_path = wav_path
        _bytes_written = 0

    # Clear queues
    while not _audio_queue.empty():
        try:
            _audio_queue.get_nowait()
        except queue.Empty:
            break

    # Clear transcription state
    with _transcript_lock:
        _transcript_segments.clear()

    with _transcription_buffer_lock:
        _transcription_buffer.clear()

    _last_transcription_time = time.time()

    # Start audio worker thread
    _audio_worker_stop.clear()
    _audio_worker_thread = threading.Thread(target=_audio_worker, daemon=True)
    _audio_worker_thread.start()

    # Start transcription worker thread
    _transcription_worker_stop.clear()
    _transcription_worker_thread = threading.Thread(target=_transcription_worker, daemon=True)
    _transcription_worker_thread.start()

    with _recording_lock:
        _recording_active = True

    # Update session state
    st.session_state.recording_active = True
    st.session_state.recording_path = str(wav_path)
    st.session_state.recording_start_time = time.time()
    st.session_state.transcription_status = "ğŸ”„ å³æ™‚è½‰éŒ„ä¸­..."
    st.session_state.last_transcript = ""
    st.session_state.last_transcript_path = ""
    st.session_state.realtime_transcript = ""
    st.session_state.segment_count = 0
    st.session_state.last_ui_update = time.time()

    print("[Transcription] Recording started")


def _stop_recording() -> None:
    """Stop recording and save transcript."""
    global _recording_active, _wav_writer, _wav_path
    global _audio_worker_thread, _transcription_worker_thread

    print("[Transcription] Stopping recording...")

    # Stop recording
    with _recording_lock:
        _recording_active = False

    # Stop worker threads
    _audio_worker_stop.set()
    if _audio_worker_thread and _audio_worker_thread.is_alive():
        _audio_worker_thread.join(timeout=2.0)

    _transcription_worker_stop.set()
    if _transcription_worker_thread and _transcription_worker_thread.is_alive():
        _transcription_worker_thread.join(timeout=3.0)

    # Close WAV file
    wav_file_path = None
    with _wav_lock:
        if _wav_writer:
            try:
                _wav_writer.close()
                print(f"[Transcription] WAV file closed: {_wav_path}")
            except Exception as exc:
                print(f"[Transcription] Error closing WAV: {exc}")
            wav_file_path = _wav_path
        _wav_writer = None
        _wav_path = None

    # Update session state
    st.session_state.recording_active = False
    st.session_state.recording_start_time = None

    # Check if we have a valid recording
    if not wav_file_path or not wav_file_path.exists():
        st.error("éŒ„éŸ³æª”æ¡ˆä¸å­˜åœ¨")
        return

    file_size = wav_file_path.stat().st_size
    if file_size <= 44:  # Only WAV header
        st.warning("éŒ„éŸ³æ™‚é–“å¤ªçŸ­ï¼Œæœªæª¢æ¸¬åˆ°éŸ³è¨Šæ•¸æ“š")
        wav_file_path.unlink()
        return

    # Get final transcript
    with _transcript_lock:
        final_transcript = "\n".join(_transcript_segments)

    if final_transcript:
        # Save transcript
        transcript_path = _save_transcript(wav_file_path, final_transcript)

        # Update session state
        st.session_state.last_transcript = final_transcript
        st.session_state.last_transcript_path = str(transcript_path)
        st.session_state.transcription_status = "âœ… è½‰éŒ„å®Œæˆ"

        print(f"[Transcription] Transcript saved: {transcript_path}")
        print(f"[Transcription] Total segments: {len(_transcript_segments)}")
    else:
        st.session_state.transcription_status = "âš ï¸ æœªæª¢æ¸¬åˆ°èªéŸ³å…§å®¹"


def _audio_worker() -> None:
    """Worker thread to write audio data to single WAV file."""
    global _bytes_written

    print("[Transcription] Audio worker started")

    while not _audio_worker_stop.is_set():
        try:
            audio_data = _audio_queue.get(timeout=0.5)
        except queue.Empty:
            continue

        with _wav_lock:
            if _wav_writer:
                try:
                    _wav_writer.writeframes(audio_data)
                    _bytes_written += len(audio_data)
                except Exception as exc:
                    print(f"[Transcription] Error writing audio: {exc}")

    print("[Transcription] Audio worker stopped")


def _transcription_worker() -> None:
    """Worker thread for background transcription every N seconds."""
    from openai import OpenAI
    global _last_transcription_time

    client = OpenAI()

    print("[Transcription] Transcription worker started")

    while not _transcription_worker_stop.is_set():
        time.sleep(0.5)  # Check every 0.5 seconds

        current_time = time.time()
        elapsed = current_time - _last_transcription_time

        # Transcribe every TRANSCRIPTION_CHUNK_DURATION seconds
        if elapsed >= TRANSCRIPTION_CHUNK_DURATION:
            # Get accumulated audio
            with _transcription_buffer_lock:
                if not _transcription_buffer:
                    _last_transcription_time = current_time
                    continue

                # Concatenate all buffers
                audio_chunk = np.concatenate(_transcription_buffer)
                _transcription_buffer.clear()

            _last_transcription_time = current_time

            # Convert to WAV bytes
            try:
                wav_bytes = _pcm_to_wav_bytes(audio_chunk, SAMPLE_RATE)

                # Create in-memory file
                wav_file = io.BytesIO(wav_bytes)
                wav_file.name = "chunk.wav"

                # Transcribe using Whisper API
                transcript = client.audio.transcriptions.create(
                    model="whisper-1",
                    file=wav_file,
                    language="zh",
                    response_format="text"
                )

                # Add to segments if not empty
                if transcript and transcript.strip():
                    with _transcript_lock:
                        _transcript_segments.append(transcript.strip())

                    print(f"[Transcription] Segment {len(_transcript_segments)}: {transcript[:50]}...")

            except Exception as exc:
                print(f"[Transcription] Error transcribing: {exc}")

    print("[Transcription] Transcription worker stopped")


def _pcm_to_wav_bytes(pcm_data: np.ndarray, sample_rate: int) -> bytes:
    """Convert PCM numpy array to WAV bytes."""
    wav_buffer = io.BytesIO()

    with wave.open(wav_buffer, "wb") as wav_file:
        wav_file.setnchannels(1)
        wav_file.setsampwidth(2)  # int16
        wav_file.setframerate(sample_rate)
        wav_file.writeframes(pcm_data.tobytes())

    return wav_buffer.getvalue()


def _save_transcript(wav_path: Path, transcript: str) -> Path:
    """
    Save transcript to text file.

    Args:
        wav_path: Path to corresponding WAV file
        transcript: Transcribed text

    Returns:
        Path to saved transcript file
    """
    # Generate transcript filename based on WAV filename
    transcript_filename = wav_path.stem + "-transcript.txt"
    transcript_path = wav_path.parent / transcript_filename

    # Create header
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    header = f"""èªéŸ³è½‰éŒ„çµæœ
æ™‚é–“ï¼š{timestamp}
éŸ³è¨Šæª”æ¡ˆï¼š{wav_path.name}
æ¡æ¨£ç‡ï¼š{SAMPLE_RATE} Hz
æ¨¡å‹ï¼šOpenAI Whisper (whisper-1)

{'=' * 60}

"""

    # Write transcript
    with open(transcript_path, "w", encoding="utf-8") as f:
        f.write(header)
        f.write(transcript)

    return transcript_path
